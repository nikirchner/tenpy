# Next up todos:

- dtype: make one enum, no dataclass and is_complex_type functions, get rid of it as arg wherever possible

- sub add mul stubs in tensors

- DiagonalTensor and Scalar subclasses
    > elementwise ops like min, max, sqrt, real, mul, ... for them
    > elementary functions sin, cos, exp, log, sqrt, ...?
    > as_diagonal_tensor
    > as_scalar
    > go through edge cases
    > scale_axis, iscale_axis

- OLD svd options (regarding degenerate singular vlaues, for example), keep them?

- pseudo-inverse, aka pinv

- we dont support real and imag, do we have as_real or sth, which takes a complex number with
  negligible imag part and gives the real part

- revisit random generation, functionality from OLD

- in-place operations
    > iadd_prefactor, iadd, isub, imul, itruediv

- I/O support (at least for Tensor, for what else?)

- label utilities
    > get_leg_idx, get_leg_idcs, has_labels, labels_are
    > change labels, e.g. set_labels, update_labels, drop_labels

- zeros_like, eye_like, ...

- make Tensor.__eq__ be allclose with a default, rather low, tolerance. doc it

- global default_backend or sth and make all backend arguments optional

- ...

- Tensor.__del__ should free up all memory. default is probably ok, but think about it again!

- write, at least stub-wise non-abelian backend


# Open design questions

- Think about mutability / views and copies
  -> mutable, e.g. via __iadd__. views are not supported, since slicing is not

- Should we care about in-place operations, where we provide a Tensor instance
  whose memory may be overwritten with the result?
  -> del and __iadd__ etc should be enough

- more metadata per leg (eg is it a physical or virtual leg, site number, ...)? a la iTensor
  -> have discussed and probably no

# After bulk of features exists

- Go through tenpy.linalg.old and see if anything is still missing

- manage namespaces, i.e. __init__.py in every module

- The following concepts have various possible names, check that use is consistent
  > **charge**, sector, ~~irrep~~
  > **dual**, conj, ingoing
  > leg, vectorspace, axis (attribute of Tensor AND arg of eg tensordot)
    proposal: Tensor.legs: list[VectorSpace]
              tensordot(..., cont1: list[int | str], cont2...)

- for function with multiple Tensor instances as inputs, should we check for compatible backends
  or deal with whatever happens if they are not compatible?

- Error handling

- sanity checks

- double-check all __str__ and __repr__

- demo notebook (basis for tutorial and tests)

- tests

- docs
